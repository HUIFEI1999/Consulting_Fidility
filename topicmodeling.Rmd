---
title: "IMDB NLP"
author: "Jiaqi Sun, Huifei Xu, Hao He, Yaquan Yang"
date: "2022-10-16"
output:
  pdf_document: default
  html_document: default 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Package usage

```{r,warning=FALSE}
library(dplyr)
library(ggplot2)
library(reshape2)
library(topicmodels)
library(wordcloud)
library(tidytext)
library(Rmpfr)
library(tidyr)
library(tm)
library(ggraph)
library(grid)
library(igraph)
library(ggraph)
library(stringr)
library(qdap)
library(ggthemes)
```


In this topic modeling assignment, we use the IMDB Movie Reviews (50K) dataset from Kaggle to analyze the review sentiment and pull out independent topic categories(i.e., movie genre) based on group of keywords.

## 1. The tidy text format

First, we import data and break down each review into a list of words. Here, we tokenizing at word level and treat each review as a separate “document”. We manually created a review number for each document since the movie titles for each review are not included in this dataset. Capitalization in reviews is kept for the initial screening. 


#note: change next following chunk!
```{r,warning=FALSE}
IMDB <- read.csv("IMDB Dataset.csv")

#IMDB <- IMDB[1:5000,] # NEED REMOVE,test use 

IMDB_df <- tibble(IMDB)
glimpse(IMDB_df)
head(IMDB_df)
```



```{r}
IMDB_df %>% 
  mutate(review_number = row_number()) ->IMDB_df 
```


```{r,warning=FALSE}

# keep the CAPS for sentiment analysis 
tidy_IMDB <- IMDB_df %>%
  unnest_tokens(word, review, to_lower = FALSE)%>% anti_join(stop_words)

tidy_IMDB

# most common words
tidy_IMDB %>% 
count(word, sort = TRUE)

#plot frequencies
tidy_IMDB %>%
  count(word, sort = TRUE) %>%
  filter(n > 9000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL,
       title = "Word Frequency of Common Words (with CAPS)") +
  xlab("Count of word")+
  ylab("Most frequent indivual word")
```

From the frequency plot above, we can see that if we keep CAPS in each review, the most common words would include some words (such as pronouns) that are useless for sentiment analysis and topic identification, so we decide to lowercase all words.




```{r}
tidy_IMDB <- IMDB_df %>%
  unnest_tokens(word, review)%>%
  anti_join(stop_words) 

tidy_IMDB
```

`Stop_words` is a data frame from tidytext package that contains English stop words from three lexicons. Except for these stop-words, we also customize a list of domain specific stop-words. 

See below:

Add "br" to be a customized stop-word as it's a leftover from html format.
Add "film", "movie" to customized stop-word as they are appearing in any review and meaningless

```{r}
data(stop_words)
stop_words <- bind_rows(tibble(word = c("br", "film", "movie", "films", "movies", "characters", "character", "story", "time","people", "watching", "scene", "scenes", "plot", "watch", "real", "cast", "director", "lot", "pretty", "10", "actors", "1", "oz", "makes", "2"), 
                               lexicon = c("custom")), 
                        stop_words)

tidy_IMDB <- tidy_IMDB %>%
  anti_join(stop_words)

tidy_IMDB
```


```{r,warning=FALSE}
tidy_IMDB %>%
  count(sentiment, word, sort = TRUE) 
```


```{r,warning=FALSE}

#The most common words in IMDB comments
tidy_IMDB %>%
  count(word, sort = TRUE) %>%
  filter(n > 6000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


```{r,warning=FALSE}

tidy_IMDB %>% 
  count(sentiment)

```

<br>
<br>

## 2. Sentiment analysis with tidy data
```{r,warning=FALSE}

bing_word_counts <- tidy_IMDB %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r,warning=FALSE}

#Words that contribute to positive and negative sentiment in IMDB
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```


```{r,warning=FALSE}
# Most common positive and negative words in IMDB

tidy_IMDB %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

```{r}
table(IMDB$sentiment)
```
The polarity function scans the subjectivity lexicon for positive and negative words, and we can further visualize the following results to compare with our sentiment tags. The subjective dictionary used by the qdap package contains about 6800 words with tags, which would be introduced here to help measuring the density of keywords

```{r}

Sys.setlocale("LC_ALL", "C")

imdb.pol<-polarity(as.character(IMDB$review))

ggplot(imdb.pol$all,aes(x=polarity,y=..density..))+
  theme_gdocs()+
  geom_histogram(binwidth=0.2,
                 fill= "#69b3a2",
                 colour="#e9ecef",
                 alpha=0.9,
                 size=0.2)+
  geom_density(size=0.75,
               fill="blue",
               adjust=1.5,
               alpha=0.4,
               color="#69b3a2")+
  ggtitle("qdap's positive and negative word ratings")

```


<br>
<br>

## 3. Analyzing word and document frequency: tf-idf
```{r,warning=FALSE}

IMDB_tf_idf <-  tidy_IMDB %>%
  count(review_number, word, sort = TRUE) %>% 
  bind_tf_idf(word, review_number, n) %>% 
  arrange((tf_idf))

IMDB_tf_idf
```


```{r}

summary(IMDB_tf_idf)
```

<br>
<br>

The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the reviews in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection.

<br>
<br>

## 4. Relationships between words: n-grams and correlations

```{r,warning=FALSE}
IMDB_bigrams <- IMDB_df %>%
  unnest_tokens(bigram, review, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

IMDB_bigrams

```

```{r,warning=FALSE}
IMDB_bigrams %>%
  count(bigram, sort = TRUE)
```

As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as of the and to be: what we call “stop-words” (see Chapter 1). This is a useful time to use tidyr’s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word.

```{r message=F,warning=F}

bigrams_separated <- IMDB_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

```

bigram and visualisation for negative, positive remarks

```{r message=F,warning=F}
get.bigram.tidy<-function(df)
{
df %>%
  unnest_tokens(bigram, review, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram)) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)
}
```


```{r,warning=FALSE}
viz.bigram.graph <- function(bigrams,layout='fr') {
  set.seed(2022)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = layout) +
    geom_edge_link( show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()+
   coord_equal(clip='off')+
   theme(plot.margin=unit(rep(2,4),'lines'))
}
```

```{r}
pos<-IMDB_df %>% filter(sentiment=='positive') 
neg<-IMDB_df %>% filter(sentiment=='negative') 

pos.bigram<-get.bigram.tidy(pos)
neg.bigram<-get.bigram.tidy(neg)
```

```{r fig.width=7,fig.height=7}
pos.bigram

ig.pos<-pos.bigram %>%
  dplyr::filter(n > 150,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
   viz.bigram.graph() +
   ggtitle('top bigrams (Freqency>=150) in positive remarks') 

ig.pos
```

top bigram graph in negative remarks

```{r fig.width=7,fig.height=7}
neg.bigram

ig.neg<-neg.bigram %>%
  filter(n > 150,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
   viz.bigram.graph() +
   ggtitle('top bigrams (Freqency>=150) in negative remarks') 

ig.neg   
```

From above two tables and graphs, we found some bigrams present
both in negative remarks and positie remarks, for example, bigram of `special` and `effects` 
occupied the first place in both ranklist!

So we'are about to show top bigrams in positive remarks only and 
negative remarks only and both in positive and negative remarks as following:

```{r fig.width=9,fig.height=9}
pos.top.bigram<-pos.bigram %>%
  filter(n > 150,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d"))

neg.top.bigram<-neg.bigram %>%
  filter(n > 150,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d"))		 

common<-inner_join(pos.top.bigram,neg.top.bigram,by=c('word1','word2')) %>%
        mutate(n=n.x+n.y) %>% 
        select(word1,word2,n) %>% 
		mutate(remark='Negative and Positive')	
		
pos.only<-anti_join(pos.top.bigram,neg.top.bigram,by=c('word1','word2')) %>%
          mutate(remark='Positive')


neg.only<-anti_join(neg.top.bigram,pos.top.bigram,by=c('word1','word2')) %>%
          mutate(remark='Negative')


allbigram<-rbind(common,pos.only,neg.only)

allbigram %>% group_by(remark) %>% slice_head(n=3)

set.seed(2022)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

allbigram %>% 
graph_from_data_frame() %>%
    ggraph(layout = 'fr') +
    geom_edge_link(aes(colour=remark), show.legend = TRUE, arrow = a) +
    geom_node_point(color = "lightblue", size = 2) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()+
    coord_equal(clip='off')+
    theme(plot.margin=unit(rep(2,4),'lines'))+
	scale_edge_colour_manual(values=c('forestgreen','black','darkred'))
```

<br>

In other analyses, we may want to work with the recombined words. tidyr’s unite() function is the inverse of separate(), and lets us recombine the columns into one. Thus, “separate/filter/count/unite” let us find the most common bigrams not containing stop-words.

```{r,warning=FALSE}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```
<br>

In other analyses you may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting n = 3:

```{r,warning=FALSE}
IMDB_df %>%
  unnest_tokens(trigram, review, token = "ngrams", n = 3) %>%
  filter(!is.na(trigram)) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

```

<br>
<br>

## 5. Converting to and from non-tidy formats

Just as some existing text mining packages provide document-term matrices as sample data or output, some algorithms expect such matrices as input. Therefore, tidytext provides cast_ verbs for converting from a tidy form to these matrices.

```{r,warning=FALSE}

IMDB_dtm <- tidy_IMDB %>%
  count(review_number, word) %>% 
  cast_dtm(review_number, word, n)

IMDB_dtm
```

<br>
<br>

## 6. Topic modeling

Latent Dirichlet allocation is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.

This function returns an object containing the full details of the model fit, such as how words are associated with topics and how topics are associated with documents.

```{r,warning=FALSE}
# choose number of topics, 6
k = 6

# set a seed so that the output of the model is predictable
IMDB_lda <- LDA(IMDB_dtm, k, control = list(seed = 1234))
IMDB_lda
#> A LDA_VEM topic model with 6 topics.
```
To start to exploring the lda model, we list the most frequent 30 terms in the topic listed, in rank order.

```{r}
imdb.topics <- topicmodels::topics(IMDB_lda, 1)
## In this case I am returning the top 30 terms.
imdb.terms <- as.data.frame(topicmodels::terms(IMDB_lda, 30), stringsAsFactors = FALSE)
imdb.terms[1:k]
```




The tidytext package provides this method for extracting the per-topic-per-word probabilities, called (“beta”), from the model.

```{r,warning=FALSE}
IMDB_topics <- tidy(IMDB_lda, matrix = "beta")
IMDB_topics
```

We could use dplyr’s slice_max() to find the 10 terms that are most common within each topic.
```{r,warning=FALSE}
top_terms <- IMDB_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

As a tidy data frame, this lends itself well to a ggplot2 visualization.

```{r,warning=FALSE}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

Besides estimating each topic as a mixture of words, LDA also models each document as a mixture of topics. We can examine the per-document-per-topic probabilities, called  (“gamma”), with the matrix = "gamma" argument to tidy().
 
```{r,warning=FALSE}
IMDB_gamma <- tidy(IMDB_lda, matrix = "gamma")
IMDB_gamma
```
## 7. Topic Modeling extension: Determine k number of topics

IN section 6, we assume the number of topics to be 6 to conduct the LDA since not pre-topic tags were provided. However, one major aspect of LDA, is that we need to know the exact k number of optimal topics for the documents. In order to accomplish this task, I am going to use a harmonic mean method to determine k based on Martin Ponweiser’s thesis：

https://research.wu.ac.at/en/publications/latent-dirichlet-allocation-in-r-3

First, we set up the fuction for computing the harmonic mean
```{r}
# The harmonic mean function
harmonicMean <- function(logLikelihoods, precision = 2000L) {
  llMed <- median(logLikelihoods)
  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
                                       prec = precision) + llMed))))
}
```

#note: change next following chunk!

In order to find the best value for k, we do this over a sequence of topic models with different vales for k. This will generate numerous topic models with different numbers of topics, creating a vector to hold the k values.
```{r}
up_k <- 6 #change Here!

#We will use a sequence of numbers from 2 to up_k
seqk <- seq(2, up_k, 1)
burnin <- 1000
iter <- 1000
keep <- 50
fitted_many <- lapply(seqk, function(k) topicmodels::LDA(IMDB_dtm, k = k,
                                                     method = "Gibbs",control = list(burnin = burnin,
                                                                         iter = iter, keep = keep) ))

# extract logliks from each topic
logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])

# compute harmonic means
hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))

```


We could visualize the results of harmonic means by plotting the results
```{r}
ldaplot <- ggplot(data.frame(seqk, hm_many), aes(x=seqk, y=hm_many)) +
  geom_path(lwd=1.5) +
  theme(text = element_text(family= NULL),
        axis.title.y=element_text(vjust=1, size=16),
        axis.title.x=element_text(vjust=-.5, size=16),
        axis.text=element_text(size=16),
        plot.title=element_text(size=20)) +
  xlab('Number of Topics') +
  ylab('Harmonic Mean') +
     annotate("text", x = 2, y = -80000, 
              label = paste("The optimal number of topics is", 
                            seqk[which.max(hm_many)])) +
  ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of IMDB", 
                          atop(italic("How many distinct topics in thE Reviews?"),
                               ""))))
ldaplot
```

The following returns the optimal number of topics:
```{r}
seqk[which.max(hm_many)]
```
























