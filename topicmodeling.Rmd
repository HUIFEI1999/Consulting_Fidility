---
title: "IMDB NLP"
author: "Jiaqi Sun, Huifei Xu, Hao He, Yaquan Yang"
date: "2022-10-16"
output:
  pdf_document: default
  html_document: default 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Package usage

```{r,warning=FALSE}
library(dplyr)
library(ggplot2)
library(reshape2)
library(topicmodels)
library(wordcloud)
library(tidytext)
library(Rmpfr)
library(tidyr)
library(tm)
library(ggraph)
library(grid)
library(igraph)
library(ggraph)
library(stringr)
library(qdap)
library(ggthemes)
library(SnowballC)
library(textstem)
```


In this topic modeling assignment, we use the IMDB Movie Reviews (50K) dataset from Kaggle to analyze the review sentiment and pull out independent topic categories(i.e., movie genre) based on group of keywords.

## 1. The tidy text format

First, we import data and break down each review into a list of words. Here, we tokenize at word level and treat each review as a separate “document”. We manually created a review number for each document since the movie titles for each review are not included in this dataset. Capitalization in reviews is kept for the initial screening. 


```{r,warning=FALSE}
IMDB <- read.csv("IMDB Dataset.csv")

IMDB_df <- tibble(IMDB)
# glimpse(IMDB_df)
# two columns: `sentiment` indicates whether the review is labeled negative or positive; `review` includes the text of each review.
head(IMDB_df)
```



```{r}
IMDB_df %>% 
  mutate(review_number = row_number()) ->IMDB_df 
```


```{r,warning=FALSE}

# keep the CAPS for sentiment analysis 
tidy_IMDB <- IMDB_df %>%
  unnest_tokens(word, review, to_lower = FALSE)%>% anti_join(stop_words)

tidy_IMDB

# most common words
tidy_IMDB %>% 
count(word, sort = TRUE)

#plot frequencies
tidy_IMDB %>%
  count(word, sort = TRUE) %>%
  filter(n > 9000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL,
       title = "Word Frequency of Common Words (with CAPS)") +
  xlab("Count of word")+
  ylab("Most frequent indivual word")
```

From the frequency plot above, we can see that if we keep CAPS in each review, the most common words would include some words (such as pronouns) that are useless for sentiment analysis and topic identification, so we decide to lowercase all words.

```{r}
tidy_IMDB <- IMDB_df %>%
  unnest_tokens(word, review)%>%
  anti_join(stop_words) 

tidy_IMDB
```

`Stop_words` is a data frame from tidytext package that contains English stop words from three lexicons. Except for these stop-words, we also customize a list of domain specific stop-words. 

See below:

Add "br" to be a customized stop-word as it's a leftover from html format.
Add "film", "movie", "em" to customized stop-word as they are appearing in any review and meaningless
 
```{r}
data(stop_words)
stop_words <- bind_rows(tibble(word = c("br", "film", "movie", "films", "movies", "characters", "character", "story", "time","people", "watching", "scene", "scenes", "plot", "watch", "real", "cast", "director", "lot", "pretty", "10", "actors", "1", "oz", "makes", "2","em"), 
                               lexicon = c("custom")), 
                        stop_words)

tidy_IMDB <- tidy_IMDB %>%
  anti_join(stop_words)

tidy_IMDB
```
After remove all the stop-words, we try stemming and lemmatization. Stemming using rules to cut words down to their stems, operating on the word by itself. Lemmatization uses knowledge about a language’s structure to reduce words down to their lemmas, the canonical or dictionary forms of words, operating on the word in its context. As an important part of NLP pipelines, these methods would help us reduce the feature space of text data and should decrease the sparsity of text data. In this way, we may have a better fitted LDA model.

```{r,warning=FALSE}
# stemming and lemmatization
library(textstem)
lemmatization <- tidy_IMDB %>% mutate(lemma = word%>% lemmatize_words())

library(SnowballC)
stemming <- tidy_IMDB %>% mutate(stem = wordStem(word))

```

```{r,warning=FALSE}
tidy_IMDB %>%
  count(sentiment, word, sort = TRUE) 
```

```{r,warning=FALSE}

#The most common words in IMDB comments
tidy_IMDB %>%
  count(word, sort = TRUE) %>%
  filter(n > 6000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL,title = "most common words before lemmatization and stemming" )

# The most common words after lemmatization
lemmatization %>%
  count(lemma, sort = TRUE) %>%
  filter(n > 6000) %>%
  mutate(lemma = reorder(lemma, n)) %>%
  ggplot(aes(n, lemma)) +
  geom_col() +
  labs(y = NULL, title = "most common words after lemmatization")

# The most common words after stemming
stemming %>%
  count(stem, sort = TRUE) %>%
  filter(n > 8000) %>%
  mutate(stem = reorder(stem, n)) %>%
  ggplot(aes(n, stem)) +
  geom_col() +
  labs(y = NULL, title = "most common words after stemming")
```
It's easy to see that stemming would convert "plays" to "plai", while in lemmatization it would be "play". 


```{r,warning=FALSE}

tidy_IMDB %>% 
  count(sentiment)

```

<br>
<br>

## 2. Sentiment analysis with tidy data
```{r,warning=FALSE}

bing_word_counts <- tidy_IMDB %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r,warning=FALSE}

#Words that contribute to positive and negative sentiment in IMDB
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```


```{r,warning=FALSE}
# Most common positive and negative words in IMDB

tidy_IMDB %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

```{r}
table(IMDB$sentiment)
```
The polarity function scans the subjectivity lexicon for positive and negative words, and we can further visualize the following results to compare with our sentiment tags. The subjective dictionary used by the qdap package contains about 6800 words with tags, which would be introduced here to help measuring the density of keywords.

```{r}

Sys.setlocale("LC_ALL", "C")

imdb.pol<-polarity(as.character(IMDB$review))

ggplot(imdb.pol$all,aes(x=polarity,y=..density..))+
  theme_gdocs()+
  geom_histogram(binwidth=0.2,
                 fill= "#69b3a2",
                 colour="#e9ecef",
                 alpha=0.9,
                 size=0.2)+
  geom_density(size=0.75,
               fill="blue",
               adjust=1.5,
               alpha=0.4,
               color="#69b3a2")+
  ggtitle("qdap's positive and negative word ratings")

```


<br>
<br>

## 3. Analyzing word and document frequency: tf-idf
```{r,warning=FALSE}

IMDB_tf_idf <-  tidy_IMDB %>%
  count(review_number, word, sort = TRUE) %>% 
  bind_tf_idf(word, review_number, n) %>% 
  arrange((tf_idf))

IMDB_tf_idf
```


```{r}

summary(IMDB_tf_idf)
```

<br>
<br>

The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the reviews in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection.

<br>
<br>

## 4. Relationships between words: n-grams and correlations

```{r,warning=FALSE}
IMDB_bigrams <- IMDB_df %>%
  unnest_tokens(bigram, review, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

IMDB_bigrams

```

```{r,warning=FALSE}
IMDB_bigrams %>%
  count(bigram, sort = TRUE)
```

As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as of the and to be: what we call “stop-words” (see Chapter 1). This is a useful time to use tidyr’s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word.

```{r message=F,warning=F}

bigrams_separated <- IMDB_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

```

bigram and visualisation for negative, positive remarks

```{r message=F,warning=F}
get.bigram.tidy<-function(df)
{
df %>%
  unnest_tokens(bigram, review, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram)) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)
}
```


```{r,warning=FALSE}
viz.bigram.graph <- function(bigrams,layout='fr') {
  set.seed(2022)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = layout) +
    geom_edge_link( show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()+
   coord_equal(clip='off')+
   theme(plot.margin=unit(rep(2,4),'lines'))
}
```

```{r}
pos<-IMDB_df %>% filter(sentiment=='positive') 
neg<-IMDB_df %>% filter(sentiment=='negative') 

pos.bigram<-get.bigram.tidy(pos)
neg.bigram<-get.bigram.tidy(neg)
```

```{r fig.width=7,fig.height=7}
pos.bigram

ig.pos<-pos.bigram %>%
  dplyr::filter(n > 150,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
   viz.bigram.graph() +
   ggtitle('top bigrams (Freqency>=150) in positive remarks') 

ig.pos
```

top bigram graph in negative remarks

```{r fig.width=7,fig.height=7}
neg.bigram

ig.neg<-neg.bigram %>%
  filter(n > 150,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
   viz.bigram.graph() +
   ggtitle('top bigrams (Freqency>=150) in negative remarks') 

ig.neg   
```

From above two tables and graphs, we found some bigrams present
both in negative remarks and positie remarks, for example, bigram of `special` and `effects` 
occupied the first place in both ranklist!

So we'are about to show top bigrams in positive remarks only and 
negative remarks only and both in positive and negative remarks as following:

```{r fig.width=9,fig.height=9}
pos.top.bigram<-pos.bigram %>%
  filter(n > 150,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d"))

neg.top.bigram<-neg.bigram %>%
  filter(n > 150,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d"))		 

common<-inner_join(pos.top.bigram,neg.top.bigram,by=c('word1','word2')) %>%
        mutate(n=n.x+n.y) %>% 
        select(word1,word2,n) %>% 
		mutate(remark='Negative and Positive')	
		
pos.only<-anti_join(pos.top.bigram,neg.top.bigram,by=c('word1','word2')) %>%
          mutate(remark='Positive')


neg.only<-anti_join(neg.top.bigram,pos.top.bigram,by=c('word1','word2')) %>%
          mutate(remark='Negative')


allbigram<-rbind(common,pos.only,neg.only)

allbigram %>% group_by(remark) %>% slice_head(n=3)

set.seed(2022)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

allbigram %>% 
graph_from_data_frame() %>%
    ggraph(layout = 'fr') +
    geom_edge_link(aes(colour=remark), show.legend = TRUE, arrow = a) +
    geom_node_point(color = "lightblue", size = 2) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()+
    coord_equal(clip='off')+
    theme(plot.margin=unit(rep(2,4),'lines'))+
	scale_edge_colour_manual(values=c('forestgreen','black','darkred'))
```

<br>

In other analyses, we may want to work with the recombined words. tidyr’s unite() function is the inverse of separate(), and lets us recombine the columns into one. Thus, “separate/filter/count/unite” let us find the most common bigrams not containing stop-words.

```{r,warning=FALSE}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```
<br>

In other analyses, we may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting n = 3:

```{r,warning=FALSE}
IMDB_df %>%
  unnest_tokens(trigram, review, token = "ngrams", n = 3) %>%
  filter(!is.na(trigram)) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

```

<br>
<br>

## 5. Converting to and from non-tidy formats

Just as some existing text mining packages provide document-term matrices as sample data or output, some algorithms expect such matrices as input. Therefore, tidytext provides cast_ verbs for converting from a tidy form to these matrices.

```{r,warning=FALSE}

IMDB_dtm <- tidy_IMDB %>%
  count(review_number, word) %>% 
  cast_dtm(review_number, word, n)

IMDB_dtm

```

<br>
<br>

## 6. Topic modeling

Latent Dirichlet allocation is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.

This function returns an object containing the full details of the model fit, such as how words are associated with topics and how topics are associated with documents.

```{r,warning=FALSE}
# choose number of topics, 6
k = 6

# set a seed so that the output of the model is predictable
IMDB_lda <- LDA(IMDB_dtm, k, control = list(seed = 1234))
IMDB_lda
#> A LDA_VEM topic model with 6 topics.
```

To start to exploring the lda model, we list the most frequent 30 terms in the topic listed, in rank order.

```{r}
imdb.topics <- topicmodels::topics(IMDB_lda, 1)
## In this case the output returns the top 30 terms.
imdb.terms <- as.data.frame(topicmodels::terms(IMDB_lda, 30), stringsAsFactors = FALSE)
imdb.terms[1:k]
```

The tidytext package provides this method for extracting the per-topic-per-word probabilities, called (“beta”), from the model.

```{r,warning=FALSE}
IMDB_topics <- tidy(IMDB_lda, matrix = "beta")
IMDB_topics
```

We could use dplyr’s slice_max() to find the 10 terms that are most common within each topic and create a visualization.
```{r,warning=FALSE}
top_terms <- IMDB_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```


Now we use data after stemming and lemmatization to see if we have an improved results for topic identification. Because stemming would convert "plays" to "plai" which can be confusing for us to interpret the modeling results, we use lemmatized data to fit the model.
```{r}
# LDA model fitting after stemming and lemmatization

lemma_dtm <- lemmatization%>% count(review_number, lemma) %>% 
  cast_dtm(review_number, lemma, n)
# lemma_dtm still has sparsity of 100%, but has less terms: 100852

lemma_lda <- LDA(lemma_dtm, k, control = list(seed = 1234))
lemma_terms <- as.data.frame(topicmodels::terms(lemma_lda, 30), stringsAsFactors = FALSE)
lemma_terms[1:k]

lemma_topics <- tidy(lemma_lda, matrix = "beta")

lemma_top_terms <- lemma_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

lemma_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()+
  labs(title = "K=6, After lemmatization LDA model results")
```
The most common words for each topic now seems more exclusive and words like "played" don't appear in the plot. However, based on the output, setting topic(K) to 6 cannot really help us distinguish one topic from the other one by eyeballing.
So we increase K to 10 given the data size of our dataset and the results improve by having less common words associated with each topic.

```{r}
# choose nnumber of topics, 10
k = 10
IMDB_lda <- LDA(IMDB_dtm, k, control = list(seed = 1234))

IMDB_topics <- tidy(IMDB_lda, matrix = "beta")
imdb.terms <- as.data.frame(topicmodels::terms(IMDB_lda, 30), stringsAsFactors = FALSE)
imdb.terms[1:k]

# LDA model fitting after stemming and lemmatization

lemma_dtm <- lemmatization%>% count(review_number, lemma) %>% 
  cast_dtm(review_number, lemma, n)
# lemma_dtm has sparsity of 100%, but has less terms: 100852

lemma_lda <- LDA(lemma_dtm, k, control = list(seed = 1234))

lemma_topics <- tidy(lemma_lda, matrix = "beta")
lemma_terms <- as.data.frame(topicmodels::terms(lemma_lda, 30), stringsAsFactors = FALSE)
lemma_terms[1:k]

lemma_top_terms <- lemma_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

lemma_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()+
  labs(title = "K=10, After lemmatization LDA model results")
```

Besides estimating each topic as a mixture of words, LDA also models each document as a mixture of topics. We can examine the per-document-per-topic probabilities, called  (“gamma”), with the matrix = "gamma" argument to tidy().
 
```{r,warning=FALSE}
IMDB_gamma <- tidy(IMDB_lda, matrix = "gamma")
IMDB_gamma
```
## 7. Topic Modeling extension: Determine k number of topics

IN section 6, we assume the number of topics to be 6 to conduct the LDA since not pre-topic tags were provided. However, one major aspect of LDA, is that we need to know the exact k number of optimal topics for the documents. In order to accomplish this task, we are going to use a harmonic mean method to determine k based on Martin Ponweiser’s thesis：

https://research.wu.ac.at/en/publications/latent-dirichlet-allocation-in-r-3

First, we set up the fuction for computing the harmonic mean
```{r}
# The harmonic mean function
harmonicMean <- function(logLikelihoods, precision = 2000L) {
  llMed <- median(logLikelihoods)
  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
                                       prec = precision) + llMed))))
}
```


In order to find the best value for k, we do this over a sequence of topic models with different vales for k. This will generate numerous topic models with different numbers of topics, creating a vector to hold the k values.
```{r}
up_k <- 10 

# We will use a sequence of numbers from 2 to up_k
seqk <- seq(2, up_k, 1)
burnin <- 1000
iter <- 1000
keep <- 50
fitted_many <- lapply(seqk, function(k) topicmodels::LDA(IMDB_dtm, k = k,
                                                     method = "Gibbs",control = list(burnin = burnin,
                                                                         iter = iter, keep = keep) ))

# extract logliks from each topic
logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])

# compute harmonic means
hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))

```


We could visualize the results of harmonic means by plotting the results
```{r,warning=FALSE}
ldaplot <- ggplot(data.frame(seqk, hm_many), aes(x=seqk, y=hm_many)) +
  geom_path(lwd=1.5) +
  theme(text = element_text(family= NULL),
        axis.title.y=element_text(vjust=1, size=16),
        axis.title.x=element_text(vjust=-.5, size=16),
        axis.text=element_text(size=16),
        plot.title=element_text(size=20)) +
  xlab('Number of Topics') +
  ylab('Harmonic Mean') +
  ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of IMDB", 
                          atop(italic("How many distinct topics in thE Reviews?"),
                               ""))))
ldaplot
```
From the plot above, we can see that the optimal number of topics for our model is 10.

The following code returns the optimal number of topics:
```{r}
seqk[which.max(hm_many)]
```
























